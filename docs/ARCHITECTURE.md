# AI Benchmark Progress Dashboard â€” Architecture

## Overview

A **data-quality-first** dashboard for tracking AI model benchmark performance over time.
Every plotted point has full provenance; missing/unverified data is explicit.

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA SOURCES                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SWE-Bench   â”‚  ARC-AGI    â”‚   Epoch     â”‚    METR     â”‚   FrontierMath     â”‚
â”‚ Leaderboard â”‚  Leaderboardâ”‚   API/CSVs  â”‚   Reports   â”‚   Papers/CSVs      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚             â”‚                â”‚
       â–¼             â–¼             â–¼             â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INGESTION LAYER (Python)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  BaseIngestor (Abstract)                                            â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ SWEBenchIngestor                                               â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ ARCAGIIngestor                                                 â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ EpochIngestor                                                  â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ METRIngestor                                                   â”‚    â”‚
â”‚  â”‚  â””â”€â”€ FrontierMathIngestor                                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Validation Pipeline (Pydantic)                                     â”‚    â”‚
â”‚  â”‚  â€¢ Schema validation        â€¢ Range checks (0-100 for %)            â”‚    â”‚
â”‚  â”‚  â€¢ Duplicate detection      â€¢ Date sanity (not future)              â”‚    â”‚
â”‚  â”‚  â€¢ Required provenance      â€¢ Trust tier assignment                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATA LAYER (DuckDB)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   models     â”‚  â”‚  benchmarks  â”‚  â”‚   results    â”‚  â”‚   sources    â”‚    â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚    â”‚
â”‚  â”‚ â€¢ model_id   â”‚  â”‚ â€¢ bench_id   â”‚  â”‚ â€¢ result_id  â”‚  â”‚ â€¢ source_id  â”‚    â”‚
â”‚  â”‚ â€¢ name       â”‚  â”‚ â€¢ name       â”‚  â”‚ â€¢ model_id   â”‚  â”‚ â€¢ source_typeâ”‚    â”‚
â”‚  â”‚ â€¢ provider   â”‚  â”‚ â€¢ category   â”‚  â”‚ â€¢ bench_id   â”‚  â”‚ â€¢ title      â”‚    â”‚
â”‚  â”‚ â€¢ family     â”‚  â”‚ â€¢ unit       â”‚  â”‚ â€¢ score      â”‚  â”‚ â€¢ url        â”‚    â”‚
â”‚  â”‚ â€¢ release_dt â”‚  â”‚ â€¢ scale_min  â”‚  â”‚ â€¢ stderr     â”‚  â”‚ â€¢ retrieved  â”‚    â”‚
â”‚  â”‚ â€¢ status     â”‚  â”‚ â€¢ scale_max  â”‚  â”‚ â€¢ source_id  â”‚  â”‚ â€¢ trust_tier â”‚    â”‚
â”‚  â”‚ â€¢ metadata   â”‚  â”‚ â€¢ higher_is  â”‚  â”‚ â€¢ eval_date  â”‚  â”‚ â€¢ parse_meth â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   _better    â”‚  â”‚ â€¢ trust_tier â”‚  â”‚ â€¢ notes      â”‚    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ notes      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  data/overrides.yml  â†’  Applied LAST for manual corrections          â”‚   â”‚
â”‚  â”‚  data/changelog.jsonl â†’  Append-only audit log of all changes        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          API LAYER (FastAPI - Read Only)                    â”‚
â”‚                                                                             â”‚
â”‚  GET /api/v1/benchmarks              List all benchmarks                    â”‚
â”‚  GET /api/v1/benchmarks/{id}/results  Results for a benchmark               â”‚
â”‚  GET /api/v1/models                  List all models                        â”‚
â”‚  GET /api/v1/models/{id}             Model details + all results            â”‚
â”‚  GET /api/v1/frontier                Frontier (best-per-date) per benchmark â”‚
â”‚  GET /api/v1/projections/{bench_id}  Projections with uncertainty           â”‚
â”‚  GET /api/v1/data-quality            Coverage, missingness, trust summary   â”‚
â”‚  GET /api/v1/changelog               Data update history                    â”‚
â”‚  GET /api/v1/export/csv              Export filtered results as CSV         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DASHBOARD (Streamlit)                                â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Page 1: OVERVIEW                                                   â”‚    â”‚
â”‚  â”‚  â€¢ Frontier best-over-time chart (all benchmarks)                   â”‚    â”‚
â”‚  â”‚  â€¢ Key stats cards                                                  â”‚    â”‚
â”‚  â”‚  â€¢ Toggles: frontier-only, official-only, date range                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Page 2: BENCHMARK EXPLORER                                         â”‚    â”‚
â”‚  â”‚  â€¢ Select benchmark â†’ time series by provider/family                â”‚    â”‚
â”‚  â”‚  â€¢ Filters: provider, model family, date range, trust tier          â”‚    â”‚
â”‚  â”‚  â€¢ Hover: citations + eval notes + trust tier                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Page 3: MODEL EXPLORER                                             â”‚    â”‚
â”‚  â”‚  â€¢ Select model â†’ all results + metadata + citations                â”‚    â”‚
â”‚  â”‚  â€¢ Comparison mode: overlay multiple models                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Page 4: PROJECTIONS                                                â”‚    â”‚
â”‚  â”‚  â€¢ Method selector: Linear / Saturation-aware                       â”‚    â”‚
â”‚  â”‚  â€¢ Window selector for fitting                                      â”‚    â”‚
â”‚  â”‚  â€¢ Uncertainty bands: 80% + 95%                                     â”‚    â”‚
â”‚  â”‚  â€¢ Disclaimer banner (always visible)                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Page 5: DATA QUALITY                                               â”‚    â”‚
â”‚  â”‚  â€¢ Coverage matrix (benchmark Ã— provider)                           â”‚    â”‚
â”‚  â”‚  â€¢ Missingness report                                               â”‚    â”‚
â”‚  â”‚  â€¢ Trust tier distribution                                          â”‚    â”‚
â”‚  â”‚  â€¢ Per-point provenance browser                                     â”‚    â”‚
â”‚  â”‚  â€¢ Changelog viewer                                                 â”‚    â”‚
â”‚  â”‚  â€¢ Last successful update timestamp                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚  GLOBAL: Export CSV | Export Chart PNG | Dark/Light Mode                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Schemas (Pydantic + DuckDB)

### 1. Model

```python
class Model(BaseModel):
    model_id: str                    # Canonical ID: "{provider}:{name}:{version}"
    name: str                        # Display name: "GPT-4o"
    provider: str                    # "OpenAI", "Anthropic", "Google DeepMind"
    family: str | None               # "GPT-4", "Claude-3", "Gemini"
    release_date: date | None        # Official release date
    release_date_source: str | None  # URL or "official announcement"
    status: Literal["verified", "unverified"]  # verified = confirmed exists
    parameter_count: float | None    # In billions
    training_compute_flop: float | None
    training_compute_notes: str | None
    metadata: dict                   # Flexible extra fields
    created_at: datetime
    updated_at: datetime
```

### 2. Benchmark

```python
class Benchmark(BaseModel):
    benchmark_id: str               # "swe_bench_verified", "arc_agi", etc.
    name: str                       # Display: "SWE-Bench Verified"
    category: str                   # "coding", "reasoning", "agentic", "math"
    description: str
    unit: str                       # "percent", "score", "problems_solved"
    scale_min: float                # 0
    scale_max: float                # 100 or 1.0
    higher_is_better: bool          # True for most benchmarks
    official_url: str | None
    paper_url: str | None
    notes: str | None               # Harness versions, known issues
    created_at: datetime
```

### 3. Result (Core Data Point)

```python
class Result(BaseModel):
    result_id: str                  # UUID or deterministic hash
    model_id: str                   # FK to Model
    benchmark_id: str               # FK to Benchmark

    # Score data
    score: float | None             # NULL if unverified/missing
    score_stderr: float | None      # Standard error if available
    score_ci_low: float | None      # Confidence interval
    score_ci_high: float | None

    # Evaluation metadata
    evaluation_date: date | None    # When the eval was run
    harness_version: str | None     # e.g., "swe-bench-v1.2"
    subset: str | None              # e.g., "verified", "full", "tier_4"

    # PROVENANCE (Required)
    source_id: str                  # FK to Source
    trust_tier: Literal["A", "B", "C"]
    evaluation_notes: str | None    # Free text

    # Audit
    created_at: datetime
    updated_at: datetime
    is_override: bool               # True if from overrides.yml
```

### 4. Source (Provenance Record)

```python
class Source(BaseModel):
    source_id: str                  # UUID
    source_type: Literal[
        "official_paper",
        "official_leaderboard",
        "official_blog",
        "third_party_eval",
        "third_party_leaderboard",
        "manual_entry"
    ]
    source_title: str               # "SWE-bench Leaderboard"
    source_url: str                 # Full URL
    retrieved_at: datetime          # UTC timestamp
    parse_method: Literal[
        "api",
        "csv_download",
        "html_scrape",
        "pdf_extract",
        "manual"
    ]
    raw_snapshot_path: str | None   # "data/raw/swe_bench_2024-01-15.csv"
    notes: str | None
    created_at: datetime
```

### 5. Trust Tier Definitions

| Tier | Definition | Examples |
|------|------------|----------|
| **A** | Official/Primary | Paper by benchmark authors, official leaderboard |
| **B** | Semi-Official | Model provider's published results, Epoch AI evals |
| **C** | Third-Party | Community runs, blog posts, unverified sources |

---

## Provenance Enforcement

### At Ingestion Time

1. **Every `Result` MUST have a `source_id`** â€” validation fails otherwise
2. **Every `Source` MUST have**:
   - `source_url` (or "manual_entry" with notes)
   - `retrieved_at` timestamp
   - `parse_method`
3. **Trust tier auto-assignment**:
   - Official leaderboards/papers â†’ Tier A
   - Epoch AI, model provider blogs â†’ Tier B
   - Everything else â†’ Tier C (can be overridden)

### In the UI

Every data point tooltip shows:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPT-4o: 33.2% Â± 2.1%                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚ ðŸ·ï¸ Trust: A (Official)                â”‚
â”‚ ðŸ“… Evaluated: 2024-11-15               â”‚
â”‚ ðŸ“„ Source: SWE-bench Leaderboard       â”‚
â”‚ ðŸ”— swe-bench.com/leaderboard           â”‚
â”‚ ðŸ“ Harness v1.2, verified subset       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Missing Data Handling

- `score = NULL` â†’ displayed as "â€”" or "Missing" in UI
- Model with `status = "unverified"` â†’ greyed out, marked with âš ï¸
- Charts show gaps (not interpolation) for missing data

---

## Adding a New Benchmark Ingestor

### Step 1: Create Ingestor Class

```python
# src/ingestors/new_benchmark.py

from .base import BaseIngestor
from src.models.schemas import Result, Source, Benchmark

class NewBenchmarkIngestor(BaseIngestor):
    """Ingestor for NewBenchmark dataset."""

    BENCHMARK_ID = "new_benchmark"
    BENCHMARK_META = Benchmark(
        benchmark_id="new_benchmark",
        name="New Benchmark",
        category="reasoning",
        description="Description here",
        unit="percent",
        scale_min=0,
        scale_max=100,
        higher_is_better=True,
        official_url="https://newbenchmark.org",
    )

    def fetch_raw(self) -> Path:
        """Download/retrieve raw data, save to data/raw/"""
        # Option A: Download CSV
        url = "https://newbenchmark.org/results.csv"
        raw_path = self.save_raw_snapshot(url, "new_benchmark")
        return raw_path

        # Option B: Load local snapshot
        return Path("data/snapshots/new_benchmark_2024-01.csv")

    def parse(self, raw_path: Path) -> list[Result]:
        """Parse raw data into Result objects."""
        df = pl.read_csv(raw_path)

        # Create source record
        source = Source(
            source_id=self.generate_source_id(),
            source_type="official_leaderboard",
            source_title="New Benchmark Leaderboard",
            source_url="https://newbenchmark.org",
            retrieved_at=datetime.utcnow(),
            parse_method="csv_download",
            raw_snapshot_path=str(raw_path),
        )
        self.register_source(source)

        results = []
        for row in df.iter_rows(named=True):
            result = Result(
                result_id=self.generate_result_id(row),
                model_id=self.normalize_model_id(row["model"]),
                benchmark_id=self.BENCHMARK_ID,
                score=row.get("score"),
                score_stderr=row.get("stderr"),
                evaluation_date=self.parse_date(row.get("date")),
                source_id=source.source_id,
                trust_tier=self.assign_trust_tier(source),
            )
            results.append(result)

        return results

    def validate(self, results: list[Result]) -> list[Result]:
        """Run benchmark-specific validation."""
        validated = []
        for r in results:
            # Range check
            if r.score is not None and not (0 <= r.score <= 100):
                self.log_warning(f"Score out of range: {r}")
                continue
            validated.append(r)
        return validated
```

### Step 2: Register in Factory

```python
# src/ingestors/__init__.py

from .new_benchmark import NewBenchmarkIngestor

INGESTORS = {
    "swe_bench_verified": SWEBenchIngestor,
    "arc_agi": ARCAGIIngestor,
    "new_benchmark": NewBenchmarkIngestor,  # Add here
    ...
}
```

### Step 3: Add Benchmark Metadata

```yaml
# data/benchmarks.yml

new_benchmark:
  name: "New Benchmark"
  category: "reasoning"
  unit: "percent"
  scale_min: 0
  scale_max: 100
  higher_is_better: true
  official_url: "https://newbenchmark.org"
```

### Step 4: Test

```bash
# Run single ingestor
python -m src.ingestors.run --benchmark new_benchmark --dry-run

# Validate output
python -m src.ingestors.run --benchmark new_benchmark --validate-only
```

---

## Data Update Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  make update-data                                               â”‚
â”‚                                                                 â”‚
â”‚  1. Backup current DB        â†’ data/backups/db_YYYYMMDD.duckdb â”‚
â”‚  2. For each ingestor:                                          â”‚
â”‚     a. fetch_raw()           â†’ data/raw/{bench}_{date}.{ext}   â”‚
â”‚     b. parse()               â†’ List[Result]                     â”‚
â”‚     c. validate()            â†’ List[Result] (filtered)          â”‚
â”‚     d. deduplicate()         â†’ Merge with existing              â”‚
â”‚  3. Apply overrides.yml      â†’ Manual corrections              â”‚
â”‚  4. Update DuckDB            â†’ Atomic transaction               â”‚
â”‚  5. Append to changelog.jsonl                                   â”‚
â”‚  6. Update "last_updated" metadata                              â”‚
â”‚                                                                 â”‚
â”‚  On ANY error: rollback to backup, report failure               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Projection Methods

### Method A: Linear Trend (Robust Baseline)

```python
def linear_projection(scores: pd.Series, dates: pd.Series,
                      window_months: int = 12) -> ProjectionResult:
    """
    Fit OLS on recent window, project forward.
    Returns point estimate + confidence intervals.
    """
    # Filter to window
    # Fit: score ~ days_since_start
    # Bootstrap for uncertainty (80%, 95% CI)
```

### Method B: Saturation-Aware (Logistic)

```python
def saturation_projection(scores: pd.Series, dates: pd.Series,
                          ceiling: float = 100) -> ProjectionResult:
    """
    Fit logistic growth model: score = ceiling / (1 + exp(-k*(t-t0)))
    Accounts for benchmark saturation.
    """
    # Fit logistic curve
    # MCMC or bootstrap for uncertainty
```

### Disclaimer (Always Shown)

> âš ï¸ **Projection Disclaimer**: These projections assume benchmark definitions,
> harnesses, and evaluation protocols remain comparable over time. They are
> mathematical extrapolations, not forecasts of real-world AI capability.
> Past trends may not continue.

---

## File Structure

```
ai-benchmark-dashboard/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md          # This file
â”‚   â””â”€â”€ ADDING_BENCHMARKS.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Downloaded snapshots (gitignored)
â”‚   â”œâ”€â”€ snapshots/               # Curated seed data (committed)
â”‚   â”œâ”€â”€ processed/               # Intermediate files
â”‚   â”œâ”€â”€ benchmark.duckdb         # Main database
â”‚   â”œâ”€â”€ overrides.yml            # Manual corrections
â”‚   â”œâ”€â”€ benchmarks.yml           # Benchmark metadata
â”‚   â””â”€â”€ changelog.jsonl          # Append-only audit log
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                # Settings, paths, constants
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py           # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # BaseIngestor ABC
â”‚   â”‚   â”œâ”€â”€ swe_bench.py
â”‚   â”‚   â”œâ”€â”€ arc_agi.py
â”‚   â”‚   â”œâ”€â”€ epoch.py
â”‚   â”‚   â”œâ”€â”€ metr.py
â”‚   â”‚   â””â”€â”€ frontier_math.py
â”‚   â”‚
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection.py        # DuckDB connection
â”‚   â”‚   â”œâ”€â”€ queries.py           # Query builders
â”‚   â”‚   â””â”€â”€ migrations.py        # Schema management
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”‚   â””â”€â”€ routes.py            # Endpoint definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ projections/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ linear.py
â”‚   â”‚   â””â”€â”€ saturation.py
â”‚   â”‚
â”‚   â””â”€â”€ dashboard/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ app.py               # Streamlit entry point
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ 1_overview.py
â”‚       â”‚   â”œâ”€â”€ 2_benchmark_explorer.py
â”‚       â”‚   â”œâ”€â”€ 3_model_explorer.py
â”‚       â”‚   â”œâ”€â”€ 4_projections.py
â”‚       â”‚   â””â”€â”€ 5_data_quality.py
â”‚       â””â”€â”€ components/
â”‚           â”œâ”€â”€ charts.py
â”‚           â”œâ”€â”€ filters.py
â”‚           â””â”€â”€ tooltips.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ update_data.py           # Main update script
â”‚   â”œâ”€â”€ validate_db.py           # Integrity checks
â”‚   â””â”€â”€ export_seed.py           # Export current DB as seed
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_schemas.py
    â”œâ”€â”€ test_ingestors.py
    â””â”€â”€ test_projections.py
```

---

## Key Design Decisions

1. **DuckDB over SQLite/Postgres**: Fast analytics, embedded, Parquet-compatible
2. **Pydantic for validation**: Type safety, clear schemas, good error messages
3. **Streamlit over Next.js**: Faster iteration, Python-native, meets all UI requirements
4. **Append-only changelog**: Full audit trail, never lose history
5. **Overrides as separate file**: Clear separation of automated vs manual data
6. **Trust tiers**: Visual hierarchy for data confidence without hiding anything

---

## Revision History

| Date | Version | Changes |
|------|---------|---------|
| 2026-01-29 | 1.0 | Initial architecture |
