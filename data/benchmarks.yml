# Benchmark definitions
# These are loaded during initialization and can be extended.

benchmarks:
  swe_bench_verified:
    name: "SWE-Bench Verified"
    category: "coding"
    description: "SWE-Bench evaluates models on real GitHub issues. The Verified subset contains 500 human-verified test cases."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://www.swebench.com/"
    paper_url: "https://arxiv.org/abs/2310.06770"

  metr_time_horizons:
    name: "METR Time Horizons"
    category: "agentic"
    description: "METR evaluates AI agents on long-horizon autonomous tasks."
    unit: "hours"
    scale_min: 0
    scale_max: 1000
    higher_is_better: true
    official_url: "https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/"
    paper_url: "https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/"

  frontiermath_tier4:
    name: "FrontierMath (Tier 4)"
    category: "math"
    description: "FrontierMath Tier 4 contains the most challenging mathematical problems."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://epoch.ai/frontiermath/tiers-1-4"
    paper_url: "https://arxiv.org/abs/2411.04872"

  arc_agi_1:
    name: "ARC-AGI 1"
    category: "reasoning"
    description: "Original Abstraction and Reasoning Corpus evaluating fluid intelligence and novel problem solving."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://arcprize.org/leaderboard"
    paper_url: "https://arxiv.org/abs/1911.01547"

  arc_agi_2:
    name: "ARC-AGI 2"
    category: "reasoning"
    description: "Updated ARC-AGI with harder tasks and improved evaluation methodology."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://arcprize.org/leaderboard"

  gpqa_diamond:
    name: "GPQA Diamond"
    category: "reasoning"
    description: "Graduate-level science questions requiring expert knowledge."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://arxiv.org/abs/2311.12022"

  mmmu:
    name: "MMMU"
    category: "multimodal"
    description: "Massive Multi-discipline Multimodal Understanding - evaluates multimodal models on college-level tasks."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://www.vals.ai/benchmarks/mmmu"
    paper_url: "https://arxiv.org/abs/2311.16502"

  zerobench:
    name: "ZeroBench"
    category: "multimodal"
    description: "Zero-shot visual understanding benchmark for evaluating vision-language models."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://zerobench.github.io/"

  humanities_last_exam:
    name: "Humanities Last Exam (No Tools)"
    category: "reasoning"
    description: "Expert-level humanities exam questions to evaluate deep knowledge and reasoning. No Tools Variant."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://scale.com/leaderboard/humanitys_last_exam_text_only"

  remote_labor_index:
    name: "Remote Labor Index"
    category: "agentic"
    description: "Evaluates AI systems on remote labor tasks measuring practical work capability."
    unit: "hours"
    scale_min: 0
    scale_max: 1000
    higher_is_better: true
    official_url: "https://scale.com/leaderboard/rli"

  epoch_capabilities_index:
    name: "Epoch Capabilities Index"
    category: "general"
    description: "Composite index of frontier AI capabilities across multiple benchmarks."
    unit: "percent"
    scale_min: 0
    scale_max: 100
    higher_is_better: true
    official_url: "https://epoch.ai/"
